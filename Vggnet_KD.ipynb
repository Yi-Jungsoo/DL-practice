{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DzPK42PpGL1X"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.optim as optim"]},{"cell_type":"code","source":["torch.cuda.is_available() # gpu 사용 확인"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzlBB8p1YT1r","executionInfo":{"status":"ok","timestamp":1701406647417,"user_tz":-540,"elapsed":4,"user":{"displayName":"이정수","userId":"18445389887963126979"}},"outputId":"74295620-5d7a-4785-f80e-6528b9ab1979"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["use_cuda=torch.cuda.is_available()\n","device=torch.device(\"cuda\" if use_cuda else \"cpu\")\n","device #cuda"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dw9n-3GrYVRj","executionInfo":{"status":"ok","timestamp":1701406647418,"user_tz":-540,"elapsed":3,"user":{"displayName":"이정수","userId":"18445389887963126979"}},"outputId":"9abb7285-6d8f-4689-c382-89c57dbc6fc8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["이미지 데이터에 대한 데이터 전처리"],"metadata":{"id":"BQWokUB3fvEs"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(32, padding = 4),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.507, 0.487, 0.441),(0.267, 0.256, 0.276))\n","])"],"metadata":{"id":"OBe4n81ZHzdC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dataset 불러오기"],"metadata":{"id":"_pEJEzgCg-aA"}},{"cell_type":"code","source":["train_dataset = datasets.CIFAR10(root= '/data', train = True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root= '/data', train = False, download=True, transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iS0zo03HduK","executionInfo":{"status":"ok","timestamp":1701406656377,"user_tz":-540,"elapsed":8961,"user":{"displayName":"이정수","userId":"18445389887963126979"}},"outputId":"106c6894-787d-4d45-87e4-3f7381ec3107"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:03<00:00, 42985285.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /data/cifar-10-python.tar.gz to /data\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["Dataloader 생성\n","\n","\n","1.   batchsize = 128\n","2.   Trainset은 shuffle, Testset은 no-shuffle\n"],"metadata":{"id":"RENTIJKahLCE"}},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"],"metadata":{"id":"-Mw7g7M4I6ZN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["하이퍼파라미터 설정"],"metadata":{"id":"DJA3MMzIlfeg"}},{"cell_type":"code","source":["epochs = 150\n","learning_rate = 0.01\n","momentum = 0.9\n","weight_decay = 0.0001"],"metadata":{"id":"bMpV8rgglTjb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Teacher model 생성 및 학습"],"metadata":{"id":"atUi6jLB4rUK"}},{"cell_type":"code","source":["class Teacher(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(Teacher, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3,32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(32,32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","\n","            nn.MaxPool2d(kernel_size=2, stride =2),\n","\n","            nn.Conv2d(32,64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(64,64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","\n","            nn.MaxPool2d(kernel_size=2, stride =2),\n","\n","\n","            nn.Conv2d(64,128,kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(128,128,kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","\n","            nn.MaxPool2d(kernel_size=2, stride =2),\n","\n","\n","            nn.Conv2d(128,256,kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(256,256,kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(256,256,kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","\n","            nn.Conv2d(256,256,kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","\n","            nn.MaxPool2d(kernel_size=2, stride =2),\n","        )\n","\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(1024,128),\n","            nn.Linear(128,10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc_layers(x)\n","        return x\n","\n","\n","teacher = Teacher().to(device) #Teacher모델 gpu에 생성\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(teacher.parameters(),lr=learning_rate,momentum=momentum,weight_decay=weight_decay)\n","\n","for epoch in range(epochs):\n","    teacher.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for idx, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        output = teacher(images)\n","        loss = criterion(output,labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(output.data, 1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels.data).cpu().sum()\n","\n","        if idx % 100 == 0:\n","            print('Epoch: {:3d} | Batch_idx: {:3d} |  Loss: {:.4f} | Acc: {:3.2f}%'.format(\n","                epoch, idx, train_loss / (idx + 1), 100. * correct / total))\n","\n","print(\"============Training finished=============\")\n","\n","\n","teacher.eval()  # 모델 평가모드\n","\n","with torch.no_grad():\n","    correct = 0\n","    val_acc = 0\n","    total = 0\n","\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = teacher(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    val_acc = 100 * correct / total\n","    print('Accuracy on the test set: {}'.format(val_acc))\n","\n","\n","torch.save({\n","    'epoch': epoch,\n","    'model_state_dict': teacher.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'loss': loss.item(),},\n","    '/content/teacher_model.pth') # 모델 epochs, weight, opimizer 상태,loss 값 등 체크포인트 저장\n","\n"],"metadata":{"id":"llEHdJWNwfi1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701410007308,"user_tz":-540,"elapsed":3350941,"user":{"displayName":"이정수","userId":"18445389887963126979"}},"outputId":"5df57b10-f353-46fc-8cd4-3a7796fa150f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:   0 | Batch_idx:   0 |  Loss: 2.3939 | Acc: 7.03%\n","Epoch:   0 | Batch_idx: 100 |  Loss: 1.7468 | Acc: 35.03%\n","Epoch:   0 | Batch_idx: 200 |  Loss: 1.5765 | Acc: 41.30%\n","Epoch:   0 | Batch_idx: 300 |  Loss: 1.4640 | Acc: 45.96%\n","Epoch:   1 | Batch_idx:   0 |  Loss: 1.1040 | Acc: 60.16%\n","Epoch:   1 | Batch_idx: 100 |  Loss: 0.9855 | Acc: 65.18%\n","Epoch:   1 | Batch_idx: 200 |  Loss: 0.9580 | Acc: 65.93%\n","Epoch:   1 | Batch_idx: 300 |  Loss: 0.9284 | Acc: 67.17%\n","Epoch:   2 | Batch_idx:   0 |  Loss: 0.7818 | Acc: 73.44%\n","Epoch:   2 | Batch_idx: 100 |  Loss: 0.7721 | Acc: 73.11%\n","Epoch:   2 | Batch_idx: 200 |  Loss: 0.7595 | Acc: 73.34%\n","Epoch:   2 | Batch_idx: 300 |  Loss: 0.7437 | Acc: 73.93%\n","Epoch:   3 | Batch_idx:   0 |  Loss: 0.6621 | Acc: 75.00%\n","Epoch:   3 | Batch_idx: 100 |  Loss: 0.6420 | Acc: 77.38%\n","Epoch:   3 | Batch_idx: 200 |  Loss: 0.6411 | Acc: 77.53%\n","Epoch:   3 | Batch_idx: 300 |  Loss: 0.6411 | Acc: 77.62%\n","Epoch:   4 | Batch_idx:   0 |  Loss: 0.5501 | Acc: 82.81%\n","Epoch:   4 | Batch_idx: 100 |  Loss: 0.5798 | Acc: 79.80%\n","Epoch:   4 | Batch_idx: 200 |  Loss: 0.5790 | Acc: 79.97%\n","Epoch:   4 | Batch_idx: 300 |  Loss: 0.5730 | Acc: 80.12%\n","Epoch:   5 | Batch_idx:   0 |  Loss: 0.6025 | Acc: 76.56%\n","Epoch:   5 | Batch_idx: 100 |  Loss: 0.5201 | Acc: 82.16%\n","Epoch:   5 | Batch_idx: 200 |  Loss: 0.5206 | Acc: 82.23%\n","Epoch:   5 | Batch_idx: 300 |  Loss: 0.5256 | Acc: 81.98%\n","Epoch:   6 | Batch_idx:   0 |  Loss: 0.5194 | Acc: 82.03%\n","Epoch:   6 | Batch_idx: 100 |  Loss: 0.4926 | Acc: 83.06%\n","Epoch:   6 | Batch_idx: 200 |  Loss: 0.4802 | Acc: 83.46%\n","Epoch:   6 | Batch_idx: 300 |  Loss: 0.4814 | Acc: 83.39%\n","Epoch:   7 | Batch_idx:   0 |  Loss: 0.3885 | Acc: 88.28%\n","Epoch:   7 | Batch_idx: 100 |  Loss: 0.4458 | Acc: 84.55%\n","Epoch:   7 | Batch_idx: 200 |  Loss: 0.4511 | Acc: 84.32%\n","Epoch:   7 | Batch_idx: 300 |  Loss: 0.4533 | Acc: 84.25%\n","Epoch:   8 | Batch_idx:   0 |  Loss: 0.4964 | Acc: 82.03%\n","Epoch:   8 | Batch_idx: 100 |  Loss: 0.4221 | Acc: 85.30%\n","Epoch:   8 | Batch_idx: 200 |  Loss: 0.4281 | Acc: 85.15%\n","Epoch:   8 | Batch_idx: 300 |  Loss: 0.4255 | Acc: 85.25%\n","Epoch:   9 | Batch_idx:   0 |  Loss: 0.3991 | Acc: 85.16%\n","Epoch:   9 | Batch_idx: 100 |  Loss: 0.3991 | Acc: 86.09%\n","Epoch:   9 | Batch_idx: 200 |  Loss: 0.4050 | Acc: 85.97%\n","Epoch:   9 | Batch_idx: 300 |  Loss: 0.4017 | Acc: 86.07%\n","Epoch:  10 | Batch_idx:   0 |  Loss: 0.5377 | Acc: 79.69%\n","Epoch:  10 | Batch_idx: 100 |  Loss: 0.3617 | Acc: 87.35%\n","Epoch:  10 | Batch_idx: 200 |  Loss: 0.3731 | Acc: 86.97%\n","Epoch:  10 | Batch_idx: 300 |  Loss: 0.3752 | Acc: 87.04%\n","Epoch:  11 | Batch_idx:   0 |  Loss: 0.3117 | Acc: 91.41%\n","Epoch:  11 | Batch_idx: 100 |  Loss: 0.3536 | Acc: 87.72%\n","Epoch:  11 | Batch_idx: 200 |  Loss: 0.3590 | Acc: 87.57%\n","Epoch:  11 | Batch_idx: 300 |  Loss: 0.3611 | Acc: 87.43%\n","Epoch:  12 | Batch_idx:   0 |  Loss: 0.2931 | Acc: 88.28%\n","Epoch:  12 | Batch_idx: 100 |  Loss: 0.3372 | Acc: 88.34%\n","Epoch:  12 | Batch_idx: 200 |  Loss: 0.3465 | Acc: 87.99%\n","Epoch:  12 | Batch_idx: 300 |  Loss: 0.3436 | Acc: 88.20%\n","Epoch:  13 | Batch_idx:   0 |  Loss: 0.3636 | Acc: 86.72%\n","Epoch:  13 | Batch_idx: 100 |  Loss: 0.3203 | Acc: 89.26%\n","Epoch:  13 | Batch_idx: 200 |  Loss: 0.3252 | Acc: 88.87%\n","Epoch:  13 | Batch_idx: 300 |  Loss: 0.3236 | Acc: 88.79%\n","Epoch:  14 | Batch_idx:   0 |  Loss: 0.3573 | Acc: 86.72%\n","Epoch:  14 | Batch_idx: 100 |  Loss: 0.3007 | Acc: 89.46%\n","Epoch:  14 | Batch_idx: 200 |  Loss: 0.3068 | Acc: 89.38%\n","Epoch:  14 | Batch_idx: 300 |  Loss: 0.3096 | Acc: 89.28%\n","Epoch:  15 | Batch_idx:   0 |  Loss: 0.3832 | Acc: 85.16%\n","Epoch:  15 | Batch_idx: 100 |  Loss: 0.2807 | Acc: 90.18%\n","Epoch:  15 | Batch_idx: 200 |  Loss: 0.2875 | Acc: 89.91%\n","Epoch:  15 | Batch_idx: 300 |  Loss: 0.2932 | Acc: 89.77%\n","Epoch:  16 | Batch_idx:   0 |  Loss: 0.1899 | Acc: 93.75%\n","Epoch:  16 | Batch_idx: 100 |  Loss: 0.2572 | Acc: 91.30%\n","Epoch:  16 | Batch_idx: 200 |  Loss: 0.2818 | Acc: 90.36%\n","Epoch:  16 | Batch_idx: 300 |  Loss: 0.2828 | Acc: 90.34%\n","Epoch:  17 | Batch_idx:   0 |  Loss: 0.2759 | Acc: 90.62%\n","Epoch:  17 | Batch_idx: 100 |  Loss: 0.2650 | Acc: 90.57%\n","Epoch:  17 | Batch_idx: 200 |  Loss: 0.2680 | Acc: 90.54%\n","Epoch:  17 | Batch_idx: 300 |  Loss: 0.2694 | Acc: 90.48%\n","Epoch:  18 | Batch_idx:   0 |  Loss: 0.2318 | Acc: 92.97%\n","Epoch:  18 | Batch_idx: 100 |  Loss: 0.2665 | Acc: 90.56%\n","Epoch:  18 | Batch_idx: 200 |  Loss: 0.2631 | Acc: 90.76%\n","Epoch:  18 | Batch_idx: 300 |  Loss: 0.2597 | Acc: 90.93%\n","Epoch:  19 | Batch_idx:   0 |  Loss: 0.1472 | Acc: 95.31%\n","Epoch:  19 | Batch_idx: 100 |  Loss: 0.2518 | Acc: 91.34%\n","Epoch:  19 | Batch_idx: 200 |  Loss: 0.2511 | Acc: 91.18%\n","Epoch:  19 | Batch_idx: 300 |  Loss: 0.2523 | Acc: 91.17%\n","Epoch:  20 | Batch_idx:   0 |  Loss: 0.2404 | Acc: 92.19%\n","Epoch:  20 | Batch_idx: 100 |  Loss: 0.2264 | Acc: 91.96%\n","Epoch:  20 | Batch_idx: 200 |  Loss: 0.2340 | Acc: 91.84%\n","Epoch:  20 | Batch_idx: 300 |  Loss: 0.2366 | Acc: 91.80%\n","Epoch:  21 | Batch_idx:   0 |  Loss: 0.2200 | Acc: 93.75%\n","Epoch:  21 | Batch_idx: 100 |  Loss: 0.2186 | Acc: 92.26%\n","Epoch:  21 | Batch_idx: 200 |  Loss: 0.2249 | Acc: 92.02%\n","Epoch:  21 | Batch_idx: 300 |  Loss: 0.2308 | Acc: 91.73%\n","Epoch:  22 | Batch_idx:   0 |  Loss: 0.1981 | Acc: 93.75%\n","Epoch:  22 | Batch_idx: 100 |  Loss: 0.2048 | Acc: 92.57%\n","Epoch:  22 | Batch_idx: 200 |  Loss: 0.2206 | Acc: 92.11%\n","Epoch:  22 | Batch_idx: 300 |  Loss: 0.2197 | Acc: 92.22%\n","Epoch:  23 | Batch_idx:   0 |  Loss: 0.1973 | Acc: 92.97%\n","Epoch:  23 | Batch_idx: 100 |  Loss: 0.2180 | Acc: 92.18%\n","Epoch:  23 | Batch_idx: 200 |  Loss: 0.2152 | Acc: 92.31%\n","Epoch:  23 | Batch_idx: 300 |  Loss: 0.2163 | Acc: 92.27%\n","Epoch:  24 | Batch_idx:   0 |  Loss: 0.2667 | Acc: 89.06%\n","Epoch:  24 | Batch_idx: 100 |  Loss: 0.2045 | Acc: 92.85%\n","Epoch:  24 | Batch_idx: 200 |  Loss: 0.2085 | Acc: 92.77%\n","Epoch:  24 | Batch_idx: 300 |  Loss: 0.2087 | Acc: 92.72%\n","Epoch:  25 | Batch_idx:   0 |  Loss: 0.2085 | Acc: 92.97%\n","Epoch:  25 | Batch_idx: 100 |  Loss: 0.1891 | Acc: 93.25%\n","Epoch:  25 | Batch_idx: 200 |  Loss: 0.1912 | Acc: 93.19%\n","Epoch:  25 | Batch_idx: 300 |  Loss: 0.1934 | Acc: 93.17%\n","Epoch:  26 | Batch_idx:   0 |  Loss: 0.2533 | Acc: 93.75%\n","Epoch:  26 | Batch_idx: 100 |  Loss: 0.1865 | Acc: 93.20%\n","Epoch:  26 | Batch_idx: 200 |  Loss: 0.1905 | Acc: 93.18%\n","Epoch:  26 | Batch_idx: 300 |  Loss: 0.1968 | Acc: 93.05%\n","Epoch:  27 | Batch_idx:   0 |  Loss: 0.2574 | Acc: 91.41%\n","Epoch:  27 | Batch_idx: 100 |  Loss: 0.1767 | Acc: 93.72%\n","Epoch:  27 | Batch_idx: 200 |  Loss: 0.1820 | Acc: 93.56%\n","Epoch:  27 | Batch_idx: 300 |  Loss: 0.1814 | Acc: 93.60%\n","Epoch:  28 | Batch_idx:   0 |  Loss: 0.1618 | Acc: 93.75%\n","Epoch:  28 | Batch_idx: 100 |  Loss: 0.1713 | Acc: 93.98%\n","Epoch:  28 | Batch_idx: 200 |  Loss: 0.1768 | Acc: 93.71%\n","Epoch:  28 | Batch_idx: 300 |  Loss: 0.1797 | Acc: 93.61%\n","Epoch:  29 | Batch_idx:   0 |  Loss: 0.1126 | Acc: 97.66%\n","Epoch:  29 | Batch_idx: 100 |  Loss: 0.1661 | Acc: 93.92%\n","Epoch:  29 | Batch_idx: 200 |  Loss: 0.1680 | Acc: 93.91%\n","Epoch:  29 | Batch_idx: 300 |  Loss: 0.1715 | Acc: 93.80%\n","Epoch:  30 | Batch_idx:   0 |  Loss: 0.2192 | Acc: 92.19%\n","Epoch:  30 | Batch_idx: 100 |  Loss: 0.1641 | Acc: 94.21%\n","Epoch:  30 | Batch_idx: 200 |  Loss: 0.1636 | Acc: 94.16%\n","Epoch:  30 | Batch_idx: 300 |  Loss: 0.1652 | Acc: 94.13%\n","Epoch:  31 | Batch_idx:   0 |  Loss: 0.1755 | Acc: 94.53%\n","Epoch:  31 | Batch_idx: 100 |  Loss: 0.1639 | Acc: 94.29%\n","Epoch:  31 | Batch_idx: 200 |  Loss: 0.1623 | Acc: 94.39%\n","Epoch:  31 | Batch_idx: 300 |  Loss: 0.1640 | Acc: 94.33%\n","Epoch:  32 | Batch_idx:   0 |  Loss: 0.2787 | Acc: 93.75%\n","Epoch:  32 | Batch_idx: 100 |  Loss: 0.1412 | Acc: 95.17%\n","Epoch:  32 | Batch_idx: 200 |  Loss: 0.1534 | Acc: 94.48%\n","Epoch:  32 | Batch_idx: 300 |  Loss: 0.1582 | Acc: 94.35%\n","Epoch:  33 | Batch_idx:   0 |  Loss: 0.1800 | Acc: 92.97%\n","Epoch:  33 | Batch_idx: 100 |  Loss: 0.1376 | Acc: 95.13%\n","Epoch:  33 | Batch_idx: 200 |  Loss: 0.1458 | Acc: 94.74%\n","Epoch:  33 | Batch_idx: 300 |  Loss: 0.1508 | Acc: 94.65%\n","Epoch:  34 | Batch_idx:   0 |  Loss: 0.0904 | Acc: 97.66%\n","Epoch:  34 | Batch_idx: 100 |  Loss: 0.1445 | Acc: 94.72%\n","Epoch:  34 | Batch_idx: 200 |  Loss: 0.1471 | Acc: 94.67%\n","Epoch:  34 | Batch_idx: 300 |  Loss: 0.1492 | Acc: 94.61%\n","Epoch:  35 | Batch_idx:   0 |  Loss: 0.1189 | Acc: 94.53%\n","Epoch:  35 | Batch_idx: 100 |  Loss: 0.1310 | Acc: 95.37%\n","Epoch:  35 | Batch_idx: 200 |  Loss: 0.1348 | Acc: 95.17%\n","Epoch:  35 | Batch_idx: 300 |  Loss: 0.1369 | Acc: 95.11%\n","Epoch:  36 | Batch_idx:   0 |  Loss: 0.0836 | Acc: 96.09%\n","Epoch:  36 | Batch_idx: 100 |  Loss: 0.1387 | Acc: 95.14%\n","Epoch:  36 | Batch_idx: 200 |  Loss: 0.1382 | Acc: 95.21%\n","Epoch:  36 | Batch_idx: 300 |  Loss: 0.1391 | Acc: 95.20%\n","Epoch:  37 | Batch_idx:   0 |  Loss: 0.1402 | Acc: 97.66%\n","Epoch:  37 | Batch_idx: 100 |  Loss: 0.1267 | Acc: 95.53%\n","Epoch:  37 | Batch_idx: 200 |  Loss: 0.1296 | Acc: 95.42%\n","Epoch:  37 | Batch_idx: 300 |  Loss: 0.1324 | Acc: 95.27%\n","Epoch:  38 | Batch_idx:   0 |  Loss: 0.1198 | Acc: 95.31%\n","Epoch:  38 | Batch_idx: 100 |  Loss: 0.1180 | Acc: 95.75%\n","Epoch:  38 | Batch_idx: 200 |  Loss: 0.1256 | Acc: 95.50%\n","Epoch:  38 | Batch_idx: 300 |  Loss: 0.1302 | Acc: 95.33%\n","Epoch:  39 | Batch_idx:   0 |  Loss: 0.0908 | Acc: 95.31%\n","Epoch:  39 | Batch_idx: 100 |  Loss: 0.1174 | Acc: 95.89%\n","Epoch:  39 | Batch_idx: 200 |  Loss: 0.1236 | Acc: 95.55%\n","Epoch:  39 | Batch_idx: 300 |  Loss: 0.1259 | Acc: 95.54%\n","Epoch:  40 | Batch_idx:   0 |  Loss: 0.0831 | Acc: 97.66%\n","Epoch:  40 | Batch_idx: 100 |  Loss: 0.1160 | Acc: 95.92%\n","Epoch:  40 | Batch_idx: 200 |  Loss: 0.1227 | Acc: 95.57%\n","Epoch:  40 | Batch_idx: 300 |  Loss: 0.1219 | Acc: 95.69%\n","Epoch:  41 | Batch_idx:   0 |  Loss: 0.1550 | Acc: 96.09%\n","Epoch:  41 | Batch_idx: 100 |  Loss: 0.1179 | Acc: 95.82%\n","Epoch:  41 | Batch_idx: 200 |  Loss: 0.1186 | Acc: 95.78%\n","Epoch:  41 | Batch_idx: 300 |  Loss: 0.1219 | Acc: 95.69%\n","Epoch:  42 | Batch_idx:   0 |  Loss: 0.0517 | Acc: 96.88%\n","Epoch:  42 | Batch_idx: 100 |  Loss: 0.1030 | Acc: 96.41%\n","Epoch:  42 | Batch_idx: 200 |  Loss: 0.1067 | Acc: 96.32%\n","Epoch:  42 | Batch_idx: 300 |  Loss: 0.1072 | Acc: 96.28%\n","Epoch:  43 | Batch_idx:   0 |  Loss: 0.1137 | Acc: 96.09%\n","Epoch:  43 | Batch_idx: 100 |  Loss: 0.1024 | Acc: 96.29%\n","Epoch:  43 | Batch_idx: 200 |  Loss: 0.1122 | Acc: 96.18%\n","Epoch:  43 | Batch_idx: 300 |  Loss: 0.1128 | Acc: 96.05%\n","Epoch:  44 | Batch_idx:   0 |  Loss: 0.1451 | Acc: 93.75%\n","Epoch:  44 | Batch_idx: 100 |  Loss: 0.0973 | Acc: 96.24%\n","Epoch:  44 | Batch_idx: 200 |  Loss: 0.1046 | Acc: 96.13%\n","Epoch:  44 | Batch_idx: 300 |  Loss: 0.1072 | Acc: 96.07%\n","Epoch:  45 | Batch_idx:   0 |  Loss: 0.0950 | Acc: 96.88%\n","Epoch:  45 | Batch_idx: 100 |  Loss: 0.0985 | Acc: 96.60%\n","Epoch:  45 | Batch_idx: 200 |  Loss: 0.1002 | Acc: 96.47%\n","Epoch:  45 | Batch_idx: 300 |  Loss: 0.0999 | Acc: 96.49%\n","Epoch:  46 | Batch_idx:   0 |  Loss: 0.0616 | Acc: 98.44%\n","Epoch:  46 | Batch_idx: 100 |  Loss: 0.0979 | Acc: 96.42%\n","Epoch:  46 | Batch_idx: 200 |  Loss: 0.1032 | Acc: 96.32%\n","Epoch:  46 | Batch_idx: 300 |  Loss: 0.1065 | Acc: 96.23%\n","Epoch:  47 | Batch_idx:   0 |  Loss: 0.0798 | Acc: 96.88%\n","Epoch:  47 | Batch_idx: 100 |  Loss: 0.0951 | Acc: 96.52%\n","Epoch:  47 | Batch_idx: 200 |  Loss: 0.0966 | Acc: 96.51%\n","Epoch:  47 | Batch_idx: 300 |  Loss: 0.0990 | Acc: 96.41%\n","Epoch:  48 | Batch_idx:   0 |  Loss: 0.0617 | Acc: 98.44%\n","Epoch:  48 | Batch_idx: 100 |  Loss: 0.0956 | Acc: 96.56%\n","Epoch:  48 | Batch_idx: 200 |  Loss: 0.1043 | Acc: 96.35%\n","Epoch:  48 | Batch_idx: 300 |  Loss: 0.1044 | Acc: 96.39%\n","Epoch:  49 | Batch_idx:   0 |  Loss: 0.1645 | Acc: 93.75%\n","Epoch:  49 | Batch_idx: 100 |  Loss: 0.1005 | Acc: 96.47%\n","Epoch:  49 | Batch_idx: 200 |  Loss: 0.0956 | Acc: 96.61%\n","Epoch:  49 | Batch_idx: 300 |  Loss: 0.0984 | Acc: 96.53%\n","Epoch:  50 | Batch_idx:   0 |  Loss: 0.1380 | Acc: 94.53%\n","Epoch:  50 | Batch_idx: 100 |  Loss: 0.0892 | Acc: 96.76%\n","Epoch:  50 | Batch_idx: 200 |  Loss: 0.0884 | Acc: 96.79%\n","Epoch:  50 | Batch_idx: 300 |  Loss: 0.0931 | Acc: 96.66%\n","Epoch:  51 | Batch_idx:   0 |  Loss: 0.1410 | Acc: 93.75%\n","Epoch:  51 | Batch_idx: 100 |  Loss: 0.0969 | Acc: 96.59%\n","Epoch:  51 | Batch_idx: 200 |  Loss: 0.0961 | Acc: 96.68%\n","Epoch:  51 | Batch_idx: 300 |  Loss: 0.0943 | Acc: 96.72%\n","Epoch:  52 | Batch_idx:   0 |  Loss: 0.0724 | Acc: 97.66%\n","Epoch:  52 | Batch_idx: 100 |  Loss: 0.0980 | Acc: 96.49%\n","Epoch:  52 | Batch_idx: 200 |  Loss: 0.0906 | Acc: 96.78%\n","Epoch:  52 | Batch_idx: 300 |  Loss: 0.0915 | Acc: 96.74%\n","Epoch:  53 | Batch_idx:   0 |  Loss: 0.0926 | Acc: 96.88%\n","Epoch:  53 | Batch_idx: 100 |  Loss: 0.0747 | Acc: 97.35%\n","Epoch:  53 | Batch_idx: 200 |  Loss: 0.0805 | Acc: 97.19%\n","Epoch:  53 | Batch_idx: 300 |  Loss: 0.0888 | Acc: 96.94%\n","Epoch:  54 | Batch_idx:   0 |  Loss: 0.1206 | Acc: 97.66%\n","Epoch:  54 | Batch_idx: 100 |  Loss: 0.0779 | Acc: 97.28%\n","Epoch:  54 | Batch_idx: 200 |  Loss: 0.0825 | Acc: 96.98%\n","Epoch:  54 | Batch_idx: 300 |  Loss: 0.0837 | Acc: 96.95%\n","Epoch:  55 | Batch_idx:   0 |  Loss: 0.0573 | Acc: 97.66%\n","Epoch:  55 | Batch_idx: 100 |  Loss: 0.0844 | Acc: 97.01%\n","Epoch:  55 | Batch_idx: 200 |  Loss: 0.0861 | Acc: 97.05%\n","Epoch:  55 | Batch_idx: 300 |  Loss: 0.0882 | Acc: 96.92%\n","Epoch:  56 | Batch_idx:   0 |  Loss: 0.0525 | Acc: 97.66%\n","Epoch:  56 | Batch_idx: 100 |  Loss: 0.0865 | Acc: 96.97%\n","Epoch:  56 | Batch_idx: 200 |  Loss: 0.0811 | Acc: 97.15%\n","Epoch:  56 | Batch_idx: 300 |  Loss: 0.0796 | Acc: 97.18%\n","Epoch:  57 | Batch_idx:   0 |  Loss: 0.1342 | Acc: 94.53%\n","Epoch:  57 | Batch_idx: 100 |  Loss: 0.0709 | Acc: 97.73%\n","Epoch:  57 | Batch_idx: 200 |  Loss: 0.0706 | Acc: 97.64%\n","Epoch:  57 | Batch_idx: 300 |  Loss: 0.0733 | Acc: 97.45%\n","Epoch:  58 | Batch_idx:   0 |  Loss: 0.0253 | Acc: 99.22%\n","Epoch:  58 | Batch_idx: 100 |  Loss: 0.0819 | Acc: 97.22%\n","Epoch:  58 | Batch_idx: 200 |  Loss: 0.0798 | Acc: 97.30%\n","Epoch:  58 | Batch_idx: 300 |  Loss: 0.0817 | Acc: 97.15%\n","Epoch:  59 | Batch_idx:   0 |  Loss: 0.0793 | Acc: 97.66%\n","Epoch:  59 | Batch_idx: 100 |  Loss: 0.0727 | Acc: 97.39%\n","Epoch:  59 | Batch_idx: 200 |  Loss: 0.0752 | Acc: 97.30%\n","Epoch:  59 | Batch_idx: 300 |  Loss: 0.0808 | Acc: 97.15%\n","Epoch:  60 | Batch_idx:   0 |  Loss: 0.0219 | Acc: 100.00%\n","Epoch:  60 | Batch_idx: 100 |  Loss: 0.0727 | Acc: 97.47%\n","Epoch:  60 | Batch_idx: 200 |  Loss: 0.0759 | Acc: 97.37%\n","Epoch:  60 | Batch_idx: 300 |  Loss: 0.0749 | Acc: 97.35%\n","Epoch:  61 | Batch_idx:   0 |  Loss: 0.1375 | Acc: 96.88%\n","Epoch:  61 | Batch_idx: 100 |  Loss: 0.0710 | Acc: 97.63%\n","Epoch:  61 | Batch_idx: 200 |  Loss: 0.0704 | Acc: 97.58%\n","Epoch:  61 | Batch_idx: 300 |  Loss: 0.0692 | Acc: 97.59%\n","Epoch:  62 | Batch_idx:   0 |  Loss: 0.1016 | Acc: 96.09%\n","Epoch:  62 | Batch_idx: 100 |  Loss: 0.0714 | Acc: 97.40%\n","Epoch:  62 | Batch_idx: 200 |  Loss: 0.0712 | Acc: 97.49%\n","Epoch:  62 | Batch_idx: 300 |  Loss: 0.0695 | Acc: 97.53%\n","Epoch:  63 | Batch_idx:   0 |  Loss: 0.0385 | Acc: 97.66%\n","Epoch:  63 | Batch_idx: 100 |  Loss: 0.0738 | Acc: 97.39%\n","Epoch:  63 | Batch_idx: 200 |  Loss: 0.0722 | Acc: 97.40%\n","Epoch:  63 | Batch_idx: 300 |  Loss: 0.0710 | Acc: 97.43%\n","Epoch:  64 | Batch_idx:   0 |  Loss: 0.0758 | Acc: 97.66%\n","Epoch:  64 | Batch_idx: 100 |  Loss: 0.0718 | Acc: 97.45%\n","Epoch:  64 | Batch_idx: 200 |  Loss: 0.0716 | Acc: 97.48%\n","Epoch:  64 | Batch_idx: 300 |  Loss: 0.0695 | Acc: 97.58%\n","Epoch:  65 | Batch_idx:   0 |  Loss: 0.0336 | Acc: 97.66%\n","Epoch:  65 | Batch_idx: 100 |  Loss: 0.0641 | Acc: 97.69%\n","Epoch:  65 | Batch_idx: 200 |  Loss: 0.0640 | Acc: 97.68%\n","Epoch:  65 | Batch_idx: 300 |  Loss: 0.0688 | Acc: 97.54%\n","Epoch:  66 | Batch_idx:   0 |  Loss: 0.0403 | Acc: 97.66%\n","Epoch:  66 | Batch_idx: 100 |  Loss: 0.0702 | Acc: 97.49%\n","Epoch:  66 | Batch_idx: 200 |  Loss: 0.0688 | Acc: 97.54%\n","Epoch:  66 | Batch_idx: 300 |  Loss: 0.0654 | Acc: 97.63%\n","Epoch:  67 | Batch_idx:   0 |  Loss: 0.0731 | Acc: 99.22%\n","Epoch:  67 | Batch_idx: 100 |  Loss: 0.0685 | Acc: 97.56%\n","Epoch:  67 | Batch_idx: 200 |  Loss: 0.0701 | Acc: 97.56%\n","Epoch:  67 | Batch_idx: 300 |  Loss: 0.0687 | Acc: 97.61%\n","Epoch:  68 | Batch_idx:   0 |  Loss: 0.0310 | Acc: 98.44%\n","Epoch:  68 | Batch_idx: 100 |  Loss: 0.0643 | Acc: 97.63%\n","Epoch:  68 | Batch_idx: 200 |  Loss: 0.0665 | Acc: 97.50%\n","Epoch:  68 | Batch_idx: 300 |  Loss: 0.0655 | Acc: 97.57%\n","Epoch:  69 | Batch_idx:   0 |  Loss: 0.1120 | Acc: 96.09%\n","Epoch:  69 | Batch_idx: 100 |  Loss: 0.0505 | Acc: 98.20%\n","Epoch:  69 | Batch_idx: 200 |  Loss: 0.0586 | Acc: 97.96%\n","Epoch:  69 | Batch_idx: 300 |  Loss: 0.0624 | Acc: 97.81%\n","Epoch:  70 | Batch_idx:   0 |  Loss: 0.0564 | Acc: 98.44%\n","Epoch:  70 | Batch_idx: 100 |  Loss: 0.0593 | Acc: 97.84%\n","Epoch:  70 | Batch_idx: 200 |  Loss: 0.0642 | Acc: 97.68%\n","Epoch:  70 | Batch_idx: 300 |  Loss: 0.0666 | Acc: 97.67%\n","Epoch:  71 | Batch_idx:   0 |  Loss: 0.0742 | Acc: 97.66%\n","Epoch:  71 | Batch_idx: 100 |  Loss: 0.0565 | Acc: 97.96%\n","Epoch:  71 | Batch_idx: 200 |  Loss: 0.0579 | Acc: 97.92%\n","Epoch:  71 | Batch_idx: 300 |  Loss: 0.0594 | Acc: 97.91%\n","Epoch:  72 | Batch_idx:   0 |  Loss: 0.0559 | Acc: 98.44%\n","Epoch:  72 | Batch_idx: 100 |  Loss: 0.0535 | Acc: 98.17%\n","Epoch:  72 | Batch_idx: 200 |  Loss: 0.0567 | Acc: 98.04%\n","Epoch:  72 | Batch_idx: 300 |  Loss: 0.0598 | Acc: 97.92%\n","Epoch:  73 | Batch_idx:   0 |  Loss: 0.0289 | Acc: 100.00%\n","Epoch:  73 | Batch_idx: 100 |  Loss: 0.0540 | Acc: 98.08%\n","Epoch:  73 | Batch_idx: 200 |  Loss: 0.0560 | Acc: 97.97%\n","Epoch:  73 | Batch_idx: 300 |  Loss: 0.0590 | Acc: 97.90%\n","Epoch:  74 | Batch_idx:   0 |  Loss: 0.0518 | Acc: 98.44%\n","Epoch:  74 | Batch_idx: 100 |  Loss: 0.0529 | Acc: 98.11%\n","Epoch:  74 | Batch_idx: 200 |  Loss: 0.0526 | Acc: 98.17%\n","Epoch:  74 | Batch_idx: 300 |  Loss: 0.0563 | Acc: 98.01%\n","Epoch:  75 | Batch_idx:   0 |  Loss: 0.0448 | Acc: 97.66%\n","Epoch:  75 | Batch_idx: 100 |  Loss: 0.0540 | Acc: 97.99%\n","Epoch:  75 | Batch_idx: 200 |  Loss: 0.0546 | Acc: 98.04%\n","Epoch:  75 | Batch_idx: 300 |  Loss: 0.0578 | Acc: 97.91%\n","Epoch:  76 | Batch_idx:   0 |  Loss: 0.0675 | Acc: 96.88%\n","Epoch:  76 | Batch_idx: 100 |  Loss: 0.0502 | Acc: 98.28%\n","Epoch:  76 | Batch_idx: 200 |  Loss: 0.0515 | Acc: 98.21%\n","Epoch:  76 | Batch_idx: 300 |  Loss: 0.0563 | Acc: 98.04%\n","Epoch:  77 | Batch_idx:   0 |  Loss: 0.0675 | Acc: 98.44%\n","Epoch:  77 | Batch_idx: 100 |  Loss: 0.0548 | Acc: 98.05%\n","Epoch:  77 | Batch_idx: 200 |  Loss: 0.0525 | Acc: 98.15%\n","Epoch:  77 | Batch_idx: 300 |  Loss: 0.0540 | Acc: 98.08%\n","Epoch:  78 | Batch_idx:   0 |  Loss: 0.0780 | Acc: 97.66%\n","Epoch:  78 | Batch_idx: 100 |  Loss: 0.0477 | Acc: 98.18%\n","Epoch:  78 | Batch_idx: 200 |  Loss: 0.0485 | Acc: 98.18%\n","Epoch:  78 | Batch_idx: 300 |  Loss: 0.0513 | Acc: 98.14%\n","Epoch:  79 | Batch_idx:   0 |  Loss: 0.0153 | Acc: 100.00%\n","Epoch:  79 | Batch_idx: 100 |  Loss: 0.0570 | Acc: 98.01%\n","Epoch:  79 | Batch_idx: 200 |  Loss: 0.0549 | Acc: 98.05%\n","Epoch:  79 | Batch_idx: 300 |  Loss: 0.0582 | Acc: 97.93%\n","Epoch:  80 | Batch_idx:   0 |  Loss: 0.0595 | Acc: 97.66%\n","Epoch:  80 | Batch_idx: 100 |  Loss: 0.0477 | Acc: 98.33%\n","Epoch:  80 | Batch_idx: 200 |  Loss: 0.0513 | Acc: 98.21%\n","Epoch:  80 | Batch_idx: 300 |  Loss: 0.0509 | Acc: 98.22%\n","Epoch:  81 | Batch_idx:   0 |  Loss: 0.0244 | Acc: 99.22%\n","Epoch:  81 | Batch_idx: 100 |  Loss: 0.0492 | Acc: 98.39%\n","Epoch:  81 | Batch_idx: 200 |  Loss: 0.0479 | Acc: 98.40%\n","Epoch:  81 | Batch_idx: 300 |  Loss: 0.0505 | Acc: 98.29%\n","Epoch:  82 | Batch_idx:   0 |  Loss: 0.0213 | Acc: 100.00%\n","Epoch:  82 | Batch_idx: 100 |  Loss: 0.0453 | Acc: 98.38%\n","Epoch:  82 | Batch_idx: 200 |  Loss: 0.0479 | Acc: 98.25%\n","Epoch:  82 | Batch_idx: 300 |  Loss: 0.0504 | Acc: 98.15%\n","Epoch:  83 | Batch_idx:   0 |  Loss: 0.0228 | Acc: 99.22%\n","Epoch:  83 | Batch_idx: 100 |  Loss: 0.0479 | Acc: 98.25%\n","Epoch:  83 | Batch_idx: 200 |  Loss: 0.0484 | Acc: 98.22%\n","Epoch:  83 | Batch_idx: 300 |  Loss: 0.0488 | Acc: 98.27%\n","Epoch:  84 | Batch_idx:   0 |  Loss: 0.0418 | Acc: 98.44%\n","Epoch:  84 | Batch_idx: 100 |  Loss: 0.0431 | Acc: 98.44%\n","Epoch:  84 | Batch_idx: 200 |  Loss: 0.0479 | Acc: 98.27%\n","Epoch:  84 | Batch_idx: 300 |  Loss: 0.0467 | Acc: 98.34%\n","Epoch:  85 | Batch_idx:   0 |  Loss: 0.0205 | Acc: 100.00%\n","Epoch:  85 | Batch_idx: 100 |  Loss: 0.0509 | Acc: 98.12%\n","Epoch:  85 | Batch_idx: 200 |  Loss: 0.0506 | Acc: 98.18%\n","Epoch:  85 | Batch_idx: 300 |  Loss: 0.0526 | Acc: 98.13%\n","Epoch:  86 | Batch_idx:   0 |  Loss: 0.0951 | Acc: 97.66%\n","Epoch:  86 | Batch_idx: 100 |  Loss: 0.0492 | Acc: 98.28%\n","Epoch:  86 | Batch_idx: 200 |  Loss: 0.0489 | Acc: 98.32%\n","Epoch:  86 | Batch_idx: 300 |  Loss: 0.0472 | Acc: 98.37%\n","Epoch:  87 | Batch_idx:   0 |  Loss: 0.0616 | Acc: 97.66%\n","Epoch:  87 | Batch_idx: 100 |  Loss: 0.0563 | Acc: 97.90%\n","Epoch:  87 | Batch_idx: 200 |  Loss: 0.0537 | Acc: 98.09%\n","Epoch:  87 | Batch_idx: 300 |  Loss: 0.0534 | Acc: 98.13%\n","Epoch:  88 | Batch_idx:   0 |  Loss: 0.0853 | Acc: 96.88%\n","Epoch:  88 | Batch_idx: 100 |  Loss: 0.0459 | Acc: 98.37%\n","Epoch:  88 | Batch_idx: 200 |  Loss: 0.0445 | Acc: 98.45%\n","Epoch:  88 | Batch_idx: 300 |  Loss: 0.0434 | Acc: 98.51%\n","Epoch:  89 | Batch_idx:   0 |  Loss: 0.0260 | Acc: 98.44%\n","Epoch:  89 | Batch_idx: 100 |  Loss: 0.0449 | Acc: 98.40%\n","Epoch:  89 | Batch_idx: 200 |  Loss: 0.0464 | Acc: 98.31%\n","Epoch:  89 | Batch_idx: 300 |  Loss: 0.0480 | Acc: 98.27%\n","Epoch:  90 | Batch_idx:   0 |  Loss: 0.0178 | Acc: 99.22%\n","Epoch:  90 | Batch_idx: 100 |  Loss: 0.0372 | Acc: 98.81%\n","Epoch:  90 | Batch_idx: 200 |  Loss: 0.0395 | Acc: 98.69%\n","Epoch:  90 | Batch_idx: 300 |  Loss: 0.0433 | Acc: 98.52%\n","Epoch:  91 | Batch_idx:   0 |  Loss: 0.0164 | Acc: 100.00%\n","Epoch:  91 | Batch_idx: 100 |  Loss: 0.0394 | Acc: 98.61%\n","Epoch:  91 | Batch_idx: 200 |  Loss: 0.0419 | Acc: 98.55%\n","Epoch:  91 | Batch_idx: 300 |  Loss: 0.0444 | Acc: 98.46%\n","Epoch:  92 | Batch_idx:   0 |  Loss: 0.1025 | Acc: 93.75%\n","Epoch:  92 | Batch_idx: 100 |  Loss: 0.0477 | Acc: 98.42%\n","Epoch:  92 | Batch_idx: 200 |  Loss: 0.0468 | Acc: 98.42%\n","Epoch:  92 | Batch_idx: 300 |  Loss: 0.0471 | Acc: 98.40%\n","Epoch:  93 | Batch_idx:   0 |  Loss: 0.0238 | Acc: 99.22%\n","Epoch:  93 | Batch_idx: 100 |  Loss: 0.0420 | Acc: 98.54%\n","Epoch:  93 | Batch_idx: 200 |  Loss: 0.0420 | Acc: 98.52%\n","Epoch:  93 | Batch_idx: 300 |  Loss: 0.0436 | Acc: 98.46%\n","Epoch:  94 | Batch_idx:   0 |  Loss: 0.1005 | Acc: 96.88%\n","Epoch:  94 | Batch_idx: 100 |  Loss: 0.0411 | Acc: 98.55%\n","Epoch:  94 | Batch_idx: 200 |  Loss: 0.0431 | Acc: 98.46%\n","Epoch:  94 | Batch_idx: 300 |  Loss: 0.0441 | Acc: 98.46%\n","Epoch:  95 | Batch_idx:   0 |  Loss: 0.0396 | Acc: 97.66%\n","Epoch:  95 | Batch_idx: 100 |  Loss: 0.0371 | Acc: 98.75%\n","Epoch:  95 | Batch_idx: 200 |  Loss: 0.0397 | Acc: 98.66%\n","Epoch:  95 | Batch_idx: 300 |  Loss: 0.0404 | Acc: 98.60%\n","Epoch:  96 | Batch_idx:   0 |  Loss: 0.0509 | Acc: 97.66%\n","Epoch:  96 | Batch_idx: 100 |  Loss: 0.0369 | Acc: 98.67%\n","Epoch:  96 | Batch_idx: 200 |  Loss: 0.0384 | Acc: 98.67%\n","Epoch:  96 | Batch_idx: 300 |  Loss: 0.0405 | Acc: 98.59%\n","Epoch:  97 | Batch_idx:   0 |  Loss: 0.0485 | Acc: 98.44%\n","Epoch:  97 | Batch_idx: 100 |  Loss: 0.0500 | Acc: 98.18%\n","Epoch:  97 | Batch_idx: 200 |  Loss: 0.0452 | Acc: 98.38%\n","Epoch:  97 | Batch_idx: 300 |  Loss: 0.0447 | Acc: 98.40%\n","Epoch:  98 | Batch_idx:   0 |  Loss: 0.0593 | Acc: 96.09%\n","Epoch:  98 | Batch_idx: 100 |  Loss: 0.0372 | Acc: 98.66%\n","Epoch:  98 | Batch_idx: 200 |  Loss: 0.0369 | Acc: 98.72%\n","Epoch:  98 | Batch_idx: 300 |  Loss: 0.0407 | Acc: 98.59%\n","Epoch:  99 | Batch_idx:   0 |  Loss: 0.1199 | Acc: 95.31%\n","Epoch:  99 | Batch_idx: 100 |  Loss: 0.0416 | Acc: 98.54%\n","Epoch:  99 | Batch_idx: 200 |  Loss: 0.0399 | Acc: 98.58%\n","Epoch:  99 | Batch_idx: 300 |  Loss: 0.0396 | Acc: 98.53%\n","Epoch: 100 | Batch_idx:   0 |  Loss: 0.0062 | Acc: 100.00%\n","Epoch: 100 | Batch_idx: 100 |  Loss: 0.0336 | Acc: 98.89%\n","Epoch: 100 | Batch_idx: 200 |  Loss: 0.0330 | Acc: 98.89%\n","Epoch: 100 | Batch_idx: 300 |  Loss: 0.0348 | Acc: 98.83%\n","Epoch: 101 | Batch_idx:   0 |  Loss: 0.0049 | Acc: 100.00%\n","Epoch: 101 | Batch_idx: 100 |  Loss: 0.0404 | Acc: 98.58%\n","Epoch: 101 | Batch_idx: 200 |  Loss: 0.0415 | Acc: 98.52%\n","Epoch: 101 | Batch_idx: 300 |  Loss: 0.0413 | Acc: 98.52%\n","Epoch: 102 | Batch_idx:   0 |  Loss: 0.1002 | Acc: 96.88%\n","Epoch: 102 | Batch_idx: 100 |  Loss: 0.0371 | Acc: 98.75%\n","Epoch: 102 | Batch_idx: 200 |  Loss: 0.0387 | Acc: 98.66%\n","Epoch: 102 | Batch_idx: 300 |  Loss: 0.0403 | Acc: 98.58%\n","Epoch: 103 | Batch_idx:   0 |  Loss: 0.0116 | Acc: 99.22%\n","Epoch: 103 | Batch_idx: 100 |  Loss: 0.0360 | Acc: 98.67%\n","Epoch: 103 | Batch_idx: 200 |  Loss: 0.0417 | Acc: 98.53%\n","Epoch: 103 | Batch_idx: 300 |  Loss: 0.0440 | Acc: 98.46%\n","Epoch: 104 | Batch_idx:   0 |  Loss: 0.0444 | Acc: 98.44%\n","Epoch: 104 | Batch_idx: 100 |  Loss: 0.0416 | Acc: 98.49%\n","Epoch: 104 | Batch_idx: 200 |  Loss: 0.0423 | Acc: 98.50%\n","Epoch: 104 | Batch_idx: 300 |  Loss: 0.0423 | Acc: 98.52%\n","Epoch: 105 | Batch_idx:   0 |  Loss: 0.0301 | Acc: 99.22%\n","Epoch: 105 | Batch_idx: 100 |  Loss: 0.0370 | Acc: 98.72%\n","Epoch: 105 | Batch_idx: 200 |  Loss: 0.0361 | Acc: 98.75%\n","Epoch: 105 | Batch_idx: 300 |  Loss: 0.0367 | Acc: 98.67%\n","Epoch: 106 | Batch_idx:   0 |  Loss: 0.0223 | Acc: 99.22%\n","Epoch: 106 | Batch_idx: 100 |  Loss: 0.0352 | Acc: 98.67%\n","Epoch: 106 | Batch_idx: 200 |  Loss: 0.0352 | Acc: 98.66%\n","Epoch: 106 | Batch_idx: 300 |  Loss: 0.0347 | Acc: 98.72%\n","Epoch: 107 | Batch_idx:   0 |  Loss: 0.0052 | Acc: 100.00%\n","Epoch: 107 | Batch_idx: 100 |  Loss: 0.0378 | Acc: 98.69%\n","Epoch: 107 | Batch_idx: 200 |  Loss: 0.0368 | Acc: 98.69%\n","Epoch: 107 | Batch_idx: 300 |  Loss: 0.0383 | Acc: 98.65%\n","Epoch: 108 | Batch_idx:   0 |  Loss: 0.0103 | Acc: 100.00%\n","Epoch: 108 | Batch_idx: 100 |  Loss: 0.0393 | Acc: 98.61%\n","Epoch: 108 | Batch_idx: 200 |  Loss: 0.0374 | Acc: 98.64%\n","Epoch: 108 | Batch_idx: 300 |  Loss: 0.0390 | Acc: 98.62%\n","Epoch: 109 | Batch_idx:   0 |  Loss: 0.0315 | Acc: 99.22%\n","Epoch: 109 | Batch_idx: 100 |  Loss: 0.0451 | Acc: 98.48%\n","Epoch: 109 | Batch_idx: 200 |  Loss: 0.0411 | Acc: 98.60%\n","Epoch: 109 | Batch_idx: 300 |  Loss: 0.0382 | Acc: 98.68%\n","Epoch: 110 | Batch_idx:   0 |  Loss: 0.0568 | Acc: 97.66%\n","Epoch: 110 | Batch_idx: 100 |  Loss: 0.0445 | Acc: 98.48%\n","Epoch: 110 | Batch_idx: 200 |  Loss: 0.0459 | Acc: 98.45%\n","Epoch: 110 | Batch_idx: 300 |  Loss: 0.0432 | Acc: 98.52%\n","Epoch: 111 | Batch_idx:   0 |  Loss: 0.0358 | Acc: 97.66%\n","Epoch: 111 | Batch_idx: 100 |  Loss: 0.0292 | Acc: 99.05%\n","Epoch: 111 | Batch_idx: 200 |  Loss: 0.0322 | Acc: 98.94%\n","Epoch: 111 | Batch_idx: 300 |  Loss: 0.0332 | Acc: 98.91%\n","Epoch: 112 | Batch_idx:   0 |  Loss: 0.0558 | Acc: 96.88%\n","Epoch: 112 | Batch_idx: 100 |  Loss: 0.0326 | Acc: 98.85%\n","Epoch: 112 | Batch_idx: 200 |  Loss: 0.0352 | Acc: 98.78%\n","Epoch: 112 | Batch_idx: 300 |  Loss: 0.0394 | Acc: 98.62%\n","Epoch: 113 | Batch_idx:   0 |  Loss: 0.0484 | Acc: 97.66%\n","Epoch: 113 | Batch_idx: 100 |  Loss: 0.0338 | Acc: 98.79%\n","Epoch: 113 | Batch_idx: 200 |  Loss: 0.0345 | Acc: 98.80%\n","Epoch: 113 | Batch_idx: 300 |  Loss: 0.0330 | Acc: 98.83%\n","Epoch: 114 | Batch_idx:   0 |  Loss: 0.0219 | Acc: 99.22%\n","Epoch: 114 | Batch_idx: 100 |  Loss: 0.0343 | Acc: 98.85%\n","Epoch: 114 | Batch_idx: 200 |  Loss: 0.0346 | Acc: 98.77%\n","Epoch: 114 | Batch_idx: 300 |  Loss: 0.0340 | Acc: 98.78%\n","Epoch: 115 | Batch_idx:   0 |  Loss: 0.0238 | Acc: 98.44%\n","Epoch: 115 | Batch_idx: 100 |  Loss: 0.0365 | Acc: 98.68%\n","Epoch: 115 | Batch_idx: 200 |  Loss: 0.0360 | Acc: 98.71%\n","Epoch: 115 | Batch_idx: 300 |  Loss: 0.0356 | Acc: 98.72%\n","Epoch: 116 | Batch_idx:   0 |  Loss: 0.0545 | Acc: 98.44%\n","Epoch: 116 | Batch_idx: 100 |  Loss: 0.0335 | Acc: 98.88%\n","Epoch: 116 | Batch_idx: 200 |  Loss: 0.0328 | Acc: 98.87%\n","Epoch: 116 | Batch_idx: 300 |  Loss: 0.0318 | Acc: 98.89%\n","Epoch: 117 | Batch_idx:   0 |  Loss: 0.0357 | Acc: 99.22%\n","Epoch: 117 | Batch_idx: 100 |  Loss: 0.0334 | Acc: 98.85%\n","Epoch: 117 | Batch_idx: 200 |  Loss: 0.0325 | Acc: 98.89%\n","Epoch: 117 | Batch_idx: 300 |  Loss: 0.0349 | Acc: 98.83%\n","Epoch: 118 | Batch_idx:   0 |  Loss: 0.0161 | Acc: 99.22%\n","Epoch: 118 | Batch_idx: 100 |  Loss: 0.0344 | Acc: 98.82%\n","Epoch: 118 | Batch_idx: 200 |  Loss: 0.0349 | Acc: 98.81%\n","Epoch: 118 | Batch_idx: 300 |  Loss: 0.0336 | Acc: 98.84%\n","Epoch: 119 | Batch_idx:   0 |  Loss: 0.0653 | Acc: 96.88%\n","Epoch: 119 | Batch_idx: 100 |  Loss: 0.0384 | Acc: 98.70%\n","Epoch: 119 | Batch_idx: 200 |  Loss: 0.0351 | Acc: 98.83%\n","Epoch: 119 | Batch_idx: 300 |  Loss: 0.0349 | Acc: 98.82%\n","Epoch: 120 | Batch_idx:   0 |  Loss: 0.0353 | Acc: 98.44%\n","Epoch: 120 | Batch_idx: 100 |  Loss: 0.0260 | Acc: 99.00%\n","Epoch: 120 | Batch_idx: 200 |  Loss: 0.0262 | Acc: 99.04%\n","Epoch: 120 | Batch_idx: 300 |  Loss: 0.0266 | Acc: 99.05%\n","Epoch: 121 | Batch_idx:   0 |  Loss: 0.0126 | Acc: 100.00%\n","Epoch: 121 | Batch_idx: 100 |  Loss: 0.0306 | Acc: 98.91%\n","Epoch: 121 | Batch_idx: 200 |  Loss: 0.0338 | Acc: 98.79%\n","Epoch: 121 | Batch_idx: 300 |  Loss: 0.0344 | Acc: 98.75%\n","Epoch: 122 | Batch_idx:   0 |  Loss: 0.0473 | Acc: 98.44%\n","Epoch: 122 | Batch_idx: 100 |  Loss: 0.0335 | Acc: 98.90%\n","Epoch: 122 | Batch_idx: 200 |  Loss: 0.0340 | Acc: 98.85%\n","Epoch: 122 | Batch_idx: 300 |  Loss: 0.0329 | Acc: 98.88%\n","Epoch: 123 | Batch_idx:   0 |  Loss: 0.0214 | Acc: 99.22%\n","Epoch: 123 | Batch_idx: 100 |  Loss: 0.0329 | Acc: 98.79%\n","Epoch: 123 | Batch_idx: 200 |  Loss: 0.0347 | Acc: 98.76%\n","Epoch: 123 | Batch_idx: 300 |  Loss: 0.0356 | Acc: 98.75%\n","Epoch: 124 | Batch_idx:   0 |  Loss: 0.0348 | Acc: 99.22%\n","Epoch: 124 | Batch_idx: 100 |  Loss: 0.0351 | Acc: 98.83%\n","Epoch: 124 | Batch_idx: 200 |  Loss: 0.0392 | Acc: 98.73%\n","Epoch: 124 | Batch_idx: 300 |  Loss: 0.0375 | Acc: 98.71%\n","Epoch: 125 | Batch_idx:   0 |  Loss: 0.0285 | Acc: 98.44%\n","Epoch: 125 | Batch_idx: 100 |  Loss: 0.0333 | Acc: 98.75%\n","Epoch: 125 | Batch_idx: 200 |  Loss: 0.0323 | Acc: 98.80%\n","Epoch: 125 | Batch_idx: 300 |  Loss: 0.0328 | Acc: 98.79%\n","Epoch: 126 | Batch_idx:   0 |  Loss: 0.0896 | Acc: 96.88%\n","Epoch: 126 | Batch_idx: 100 |  Loss: 0.0338 | Acc: 98.72%\n","Epoch: 126 | Batch_idx: 200 |  Loss: 0.0305 | Acc: 98.88%\n","Epoch: 126 | Batch_idx: 300 |  Loss: 0.0304 | Acc: 98.92%\n","Epoch: 127 | Batch_idx:   0 |  Loss: 0.0130 | Acc: 99.22%\n","Epoch: 127 | Batch_idx: 100 |  Loss: 0.0264 | Acc: 99.01%\n","Epoch: 127 | Batch_idx: 200 |  Loss: 0.0288 | Acc: 98.95%\n","Epoch: 127 | Batch_idx: 300 |  Loss: 0.0320 | Acc: 98.85%\n","Epoch: 128 | Batch_idx:   0 |  Loss: 0.0410 | Acc: 98.44%\n","Epoch: 128 | Batch_idx: 100 |  Loss: 0.0303 | Acc: 98.95%\n","Epoch: 128 | Batch_idx: 200 |  Loss: 0.0280 | Acc: 99.00%\n","Epoch: 128 | Batch_idx: 300 |  Loss: 0.0271 | Acc: 99.06%\n","Epoch: 129 | Batch_idx:   0 |  Loss: 0.0506 | Acc: 99.22%\n","Epoch: 129 | Batch_idx: 100 |  Loss: 0.0280 | Acc: 98.93%\n","Epoch: 129 | Batch_idx: 200 |  Loss: 0.0268 | Acc: 98.98%\n","Epoch: 129 | Batch_idx: 300 |  Loss: 0.0281 | Acc: 98.96%\n","Epoch: 130 | Batch_idx:   0 |  Loss: 0.0659 | Acc: 97.66%\n","Epoch: 130 | Batch_idx: 100 |  Loss: 0.0313 | Acc: 98.92%\n","Epoch: 130 | Batch_idx: 200 |  Loss: 0.0299 | Acc: 98.97%\n","Epoch: 130 | Batch_idx: 300 |  Loss: 0.0329 | Acc: 98.85%\n","Epoch: 131 | Batch_idx:   0 |  Loss: 0.0637 | Acc: 97.66%\n","Epoch: 131 | Batch_idx: 100 |  Loss: 0.0315 | Acc: 98.89%\n","Epoch: 131 | Batch_idx: 200 |  Loss: 0.0283 | Acc: 99.04%\n","Epoch: 131 | Batch_idx: 300 |  Loss: 0.0281 | Acc: 99.06%\n","Epoch: 132 | Batch_idx:   0 |  Loss: 0.0094 | Acc: 100.00%\n","Epoch: 132 | Batch_idx: 100 |  Loss: 0.0292 | Acc: 98.99%\n","Epoch: 132 | Batch_idx: 200 |  Loss: 0.0311 | Acc: 98.94%\n","Epoch: 132 | Batch_idx: 300 |  Loss: 0.0325 | Acc: 98.91%\n","Epoch: 133 | Batch_idx:   0 |  Loss: 0.0365 | Acc: 98.44%\n","Epoch: 133 | Batch_idx: 100 |  Loss: 0.0319 | Acc: 98.86%\n","Epoch: 133 | Batch_idx: 200 |  Loss: 0.0314 | Acc: 98.89%\n","Epoch: 133 | Batch_idx: 300 |  Loss: 0.0325 | Acc: 98.84%\n","Epoch: 134 | Batch_idx:   0 |  Loss: 0.0309 | Acc: 98.44%\n","Epoch: 134 | Batch_idx: 100 |  Loss: 0.0363 | Acc: 98.75%\n","Epoch: 134 | Batch_idx: 200 |  Loss: 0.0325 | Acc: 98.85%\n","Epoch: 134 | Batch_idx: 300 |  Loss: 0.0335 | Acc: 98.82%\n","Epoch: 135 | Batch_idx:   0 |  Loss: 0.0305 | Acc: 98.44%\n","Epoch: 135 | Batch_idx: 100 |  Loss: 0.0260 | Acc: 99.09%\n","Epoch: 135 | Batch_idx: 200 |  Loss: 0.0283 | Acc: 99.02%\n","Epoch: 135 | Batch_idx: 300 |  Loss: 0.0318 | Acc: 98.89%\n","Epoch: 136 | Batch_idx:   0 |  Loss: 0.0115 | Acc: 100.00%\n","Epoch: 136 | Batch_idx: 100 |  Loss: 0.0311 | Acc: 98.96%\n","Epoch: 136 | Batch_idx: 200 |  Loss: 0.0284 | Acc: 99.03%\n","Epoch: 136 | Batch_idx: 300 |  Loss: 0.0293 | Acc: 98.99%\n","Epoch: 137 | Batch_idx:   0 |  Loss: 0.0147 | Acc: 99.22%\n","Epoch: 137 | Batch_idx: 100 |  Loss: 0.0259 | Acc: 99.09%\n","Epoch: 137 | Batch_idx: 200 |  Loss: 0.0289 | Acc: 98.96%\n","Epoch: 137 | Batch_idx: 300 |  Loss: 0.0314 | Acc: 98.89%\n","Epoch: 138 | Batch_idx:   0 |  Loss: 0.0218 | Acc: 99.22%\n","Epoch: 138 | Batch_idx: 100 |  Loss: 0.0352 | Acc: 98.70%\n","Epoch: 138 | Batch_idx: 200 |  Loss: 0.0316 | Acc: 98.83%\n","Epoch: 138 | Batch_idx: 300 |  Loss: 0.0315 | Acc: 98.87%\n","Epoch: 139 | Batch_idx:   0 |  Loss: 0.0085 | Acc: 100.00%\n","Epoch: 139 | Batch_idx: 100 |  Loss: 0.0286 | Acc: 99.02%\n","Epoch: 139 | Batch_idx: 200 |  Loss: 0.0329 | Acc: 98.87%\n","Epoch: 139 | Batch_idx: 300 |  Loss: 0.0322 | Acc: 98.90%\n","Epoch: 140 | Batch_idx:   0 |  Loss: 0.0221 | Acc: 99.22%\n","Epoch: 140 | Batch_idx: 100 |  Loss: 0.0260 | Acc: 99.16%\n","Epoch: 140 | Batch_idx: 200 |  Loss: 0.0281 | Acc: 99.07%\n","Epoch: 140 | Batch_idx: 300 |  Loss: 0.0283 | Acc: 99.08%\n","Epoch: 141 | Batch_idx:   0 |  Loss: 0.0078 | Acc: 100.00%\n","Epoch: 141 | Batch_idx: 100 |  Loss: 0.0275 | Acc: 99.01%\n","Epoch: 141 | Batch_idx: 200 |  Loss: 0.0272 | Acc: 99.03%\n","Epoch: 141 | Batch_idx: 300 |  Loss: 0.0274 | Acc: 99.04%\n","Epoch: 142 | Batch_idx:   0 |  Loss: 0.0211 | Acc: 98.44%\n","Epoch: 142 | Batch_idx: 100 |  Loss: 0.0280 | Acc: 98.98%\n","Epoch: 142 | Batch_idx: 200 |  Loss: 0.0291 | Acc: 98.97%\n","Epoch: 142 | Batch_idx: 300 |  Loss: 0.0302 | Acc: 98.97%\n","Epoch: 143 | Batch_idx:   0 |  Loss: 0.0167 | Acc: 100.00%\n","Epoch: 143 | Batch_idx: 100 |  Loss: 0.0306 | Acc: 98.94%\n","Epoch: 143 | Batch_idx: 200 |  Loss: 0.0304 | Acc: 98.98%\n","Epoch: 143 | Batch_idx: 300 |  Loss: 0.0310 | Acc: 98.97%\n","Epoch: 144 | Batch_idx:   0 |  Loss: 0.0097 | Acc: 100.00%\n","Epoch: 144 | Batch_idx: 100 |  Loss: 0.0254 | Acc: 99.10%\n","Epoch: 144 | Batch_idx: 200 |  Loss: 0.0244 | Acc: 99.10%\n","Epoch: 144 | Batch_idx: 300 |  Loss: 0.0269 | Acc: 99.02%\n","Epoch: 145 | Batch_idx:   0 |  Loss: 0.0285 | Acc: 99.22%\n","Epoch: 145 | Batch_idx: 100 |  Loss: 0.0323 | Acc: 98.86%\n","Epoch: 145 | Batch_idx: 200 |  Loss: 0.0322 | Acc: 98.87%\n","Epoch: 145 | Batch_idx: 300 |  Loss: 0.0327 | Acc: 98.87%\n","Epoch: 146 | Batch_idx:   0 |  Loss: 0.0086 | Acc: 100.00%\n","Epoch: 146 | Batch_idx: 100 |  Loss: 0.0282 | Acc: 99.04%\n","Epoch: 146 | Batch_idx: 200 |  Loss: 0.0275 | Acc: 99.06%\n","Epoch: 146 | Batch_idx: 300 |  Loss: 0.0279 | Acc: 99.05%\n","Epoch: 147 | Batch_idx:   0 |  Loss: 0.0077 | Acc: 100.00%\n","Epoch: 147 | Batch_idx: 100 |  Loss: 0.0247 | Acc: 99.09%\n","Epoch: 147 | Batch_idx: 200 |  Loss: 0.0256 | Acc: 99.06%\n","Epoch: 147 | Batch_idx: 300 |  Loss: 0.0282 | Acc: 99.00%\n","Epoch: 148 | Batch_idx:   0 |  Loss: 0.0167 | Acc: 99.22%\n","Epoch: 148 | Batch_idx: 100 |  Loss: 0.0244 | Acc: 99.13%\n","Epoch: 148 | Batch_idx: 200 |  Loss: 0.0255 | Acc: 99.09%\n","Epoch: 148 | Batch_idx: 300 |  Loss: 0.0274 | Acc: 99.09%\n","Epoch: 149 | Batch_idx:   0 |  Loss: 0.0144 | Acc: 100.00%\n","Epoch: 149 | Batch_idx: 100 |  Loss: 0.0251 | Acc: 99.19%\n","Epoch: 149 | Batch_idx: 200 |  Loss: 0.0256 | Acc: 99.18%\n","Epoch: 149 | Batch_idx: 300 |  Loss: 0.0253 | Acc: 99.16%\n","============Training finished=============\n","Accuracy on the test set: 88.96\n"]}]},{"cell_type":"markdown","source":["모델 학습 함수"],"metadata":{"id":"hEdodhiBLW-5"}},{"cell_type":"code","source":["class Student(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(Student,self).__init__()\n","\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3,16,kernel_size=3, padding =1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","\n","            nn.Conv2d(16,16,kernel_size=3, padding =1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        )\n","        self.fc_layers = nn.Linear(1024,10)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x= self.fc_layers(x)\n","        return x\n","\n","\n","student = Student().to(device)  #Student모델 gpu에 생성\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(student.parameters(),lr=learning_rate,momentum=momentum,weight_decay=weight_decay)\n","\n","for epoch in range(epochs):\n","    student.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for idx, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","\n","        output = student(images)\n","        loss = criterion(output,labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(output.data, 1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels.data).cpu().sum()\n","\n","        if idx % 100 == 0:\n","            print('Epoch: {:3d} | Batch_idx: {:3d} |  Loss: {:.4f} | Acc: {:3.2f}%'.format(\n","                epoch, idx, train_loss / (idx + 1), 100. * correct / total))\n","\n","print(\"============Training finished=============\")\n","\n","student.eval()  # 모델 평가모드\n","with torch.no_grad(): #no gradient\n","    correct = 0\n","    val_acc = 0\n","    total = 0\n","\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = student(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    val_acc = 100 * correct / total\n","    print('Accuracy on the test set: {}'.format(val_acc))\n","\n","torch.save({\n","    'epoch': epoch,\n","    'model_state_dict': student.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'loss': loss.item(),\n","    }, '/content/student_model.pth') # 모델 epochs, weight, opimizer 상태,loss 값 등 체크포인트 저장"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"xhYWGPi8jk1g","executionInfo":{"status":"error","timestamp":1701413197296,"user_tz":-540,"elapsed":2,"user":{"displayName":"이정수","userId":"18445389887963126979"}},"outputId":"46353575-5df7-40a3-a22e-87af9766df39"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1e3a6ae5849e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mStudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStudent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.features = nn.Sequential(\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"markdown","source":["Knowledge Distillation Train\n","\n","*  10 layers 모델(Teacher) > 2 layer 모델(Student)\n","\n","\n","\n"],"metadata":{"id":"pmnevp0AOG1T"}},{"cell_type":"code","source":["trained_teacher = Teacher().to(device)\n","model_ckp = torch.load('/content/teacher_model.pth')\n","trained_teacher.load_state_dict(model_ckp['model_state_dict']) #teacher model 새로 생성후 teacher_checkpoint load를 가져와서 trained_teacher에 적용\n","\n","\n","lambda_ = 0.0001 #Knowledge distillation을 위한 parameters (lamda_, T)\n","T = 4.5\n","kl_div_loss = nn.KLDivLoss() #Knowledge distillation을 위한 Cost function\n","\n","\n","for epoch in range(epochs):\n","    student.train() #위에 train된 student 모델이 아니라 위에 만든 student를 그대로 다시 적용\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for idx, (images, labels) in enumerate(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        s_output = student(images)\n","        t_output =trained_teacher(images)\n","\n","        loss_SL = criterion(s_output, labels) # Standard Learning loss\n","        loss_KD = kl_div_loss(F.log_softmax(s_output / T, dim=1),\n","                            F.softmax(t_output / T, dim=1))\n","        loss = (1 - lambda_) * loss_SL + lambda_ * T * T * loss_KD  # total_loss = (1 −λ)⋅loss_SL +λ⋅T^2 ⋅loss_KD)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(s_output.data, 1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels.data).cpu().sum()\n","\n","        if idx % 100 == 0:\n","            print('Epoch: {:3d} | Batch_idx: {:3d} |  Loss: {:.4f} | Acc: {:3.2f}%'.format(\n","                epoch, idx, train_loss / (idx + 1), 100. * correct / total))\n","\n","print(\"============Training finished=============\")\n","\n","\n","student.eval()  # 모델 평가모드\n","with torch.no_grad(): #no gradient\n","    correct = 0\n","    val_acc = 0\n","    total = 0\n","\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = student(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        val_acc = 100 * correct / total\n","        print('Accuracy on the test set: {}'.format(val_acc))\n","\n","\n","torch.save({\n","    'epoch': epoch,\n","    'model_state_dict': student.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    'loss': loss.item(),},\n","    '/content/KD_model.pth') # KD 적용한 student 모델의 epochs, weight, opimizer 상태,loss 값 등 체크포인트 저장\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"Bd2Sf9TVs2p3","executionInfo":{"status":"error","timestamp":1701413205123,"user_tz":-540,"elapsed":332,"user":{"displayName":"이정수","userId":"18445389887963126979"}},"outputId":"aa843381-942c-4c32-ffeb-f5b67daf2d98"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8f2019094a87>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_teacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTeacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_ckp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/teacher_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrained_teacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ckp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#teacher model 새로 생성후 teacher_checkpoint load를 가져와서 trained_teacher에 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Teacher' is not defined"]}]}]}